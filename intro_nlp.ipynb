{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Background\n",
    "\n",
    "\n",
    "### 1.1 What is NLP? \n",
    "\n",
    "Natural Language Processing, or NLP, is an area of computer science that focuses on developing techniques to produce machine-driven analyses of text.\n",
    "\n",
    "### 1.2 Why is Natural Language Processing Important? \n",
    "\n",
    "NLP expands the sheer amount of data that can be used for insight. Since so much of the data we have available is in the form of text, this is extremely important to data science!\n",
    "\n",
    "A specific common application of NLP is each time you use a language conversion tool. The techniques used to accurately convert text from one language to another very much falls under the umbrella of \"natural language processing.\"\n",
    "\n",
    "### 1.3 Why is NLP a \"hard\" problem? \n",
    "\n",
    "Language is inherently ambiguous. Once person's interpretation of a sentence may very well differ from another person's interpretation. Because of this inability to consistently be clear, it's hard to have an NLP technique that works perfectly. \n",
    "\n",
    "### 1.4 Glossary\n",
    "\n",
    "Here is some common terminology that we'll encounter throughout the workshop:\n",
    "\n",
    "<b>Corpus: </b> (Plural: Corpora) a collection of written texts that serve as our datasets.\n",
    "\n",
    "<b>nltk: </b> (Natural Language Toolkit) the python module we'll be using repeatedly; it has a lot of useful built-in NLP techniques.\n",
    "\n",
    "<b>Token: </b> a string of contiguous characters between two spaces, or between a space and punctuation marks. A token can also be an integer, real, or a number with a colon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Sentiment Analysis  \n",
    "\n",
    "So you might be asking, what exactly is \"sentiment analysis\"? \n",
    "\n",
    "Well, sentiment analysis involves building a system to collect and determine the emotional tone behind words. This is important because it allows you to gain an understanding of the attitudes, opinions and emotions of the people in your data. \n",
    "\n",
    "At a high level, sentiment analysis involves Natural language processing and artificial intelligence by taking the actual text element, transforming it into a format that a machine can read, and using statistics to determine the actual sentiment.\n",
    "\n",
    "### 2.1 Preparing the Data \n",
    "\n",
    "To accomplish sentiment analysis computationally, we have to use techniques that will allow us to learn from data that's already been labeled. \n",
    "\n",
    "So what's the first step? Formatting the data so that we can actually apply NLP techniques. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def format_sentence(sent):\n",
    "    return({word: True for word in nltk.word_tokenize(sent)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `format_sentence` changes a piece of text, in this case a tweet, into a dictionary of words mapped to True booleans. Though not obvious from this function alone, this will eventually allow us to train  our prediction model by splitting the text into its tokens, i.e. <i>tokenizing</i> the text.\n",
    "\n",
    "{'!': True, 'animals': True, 'are': True, 'the': True, 'ever': True, 'Dogs': True, 'best': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll learn about why this format is important is section 2.2.\n",
    "\n",
    "Using the data on the github repo, we'll actually format the positively and negatively labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = []\n",
    "with open(\"./pos_tweets.txt\") as f:\n",
    "    for i in f: \n",
    "        pos.append([format_sentence(i), 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg = []\n",
    "with open(\"./neg_tweets.txt\") as f:\n",
    "    for i in f: \n",
    "        neg.append([format_sentence(i), 'neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Training Data\n",
    "\n",
    "Next, we'll split the labeled data we have into two pieces, one that can \"train\" data and the other to give us insight on how well our model is performing. The training data will inform our model on which features are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = pos[:int((.9)*len(pos))] + neg[:int((.9)*len(neg))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Test Data\n",
    "\n",
    "We won't use the test data until the very end of this section, but nevertheless, we save the last 10% of the data to check the accuracy of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pos[int((.1)*len(pos)):] + neg[int((.1)*len(neg)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building a Classifier\n",
    "\n",
    "All NLTK classifiers work with feature structures, which can be simple dictionaries mapping a feature name to a feature value. In this example, weâ€™ve used a simple bag of words model where every word is a feature name with a value of True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To see which features informed our model the most, we can run this line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Classification\n",
    "\n",
    "Just to see that our model works, let's try the classifier out with a positive example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example1 = \"this workshop is awesome.\"\n",
    "\n",
    "print(classifier.classify(format_sentence(example1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a negative example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example2 = \"this workshop is awful.\"\n",
    "\n",
    "print(classifier.classify(format_sentence(example2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Accuracy\n",
    "\n",
    "Now, there's no point in building a model if it doesn't work well. Luckily, once again, nltk comes to the rescue with a built in feature that allows us find the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out it works decently well!\n",
    "\n",
    "But it could be better! I think we can agree that the data is kind of messy - there are typos, abbreviations, grammatical errors of all sorts... So how do we handle that? Can we handle that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Regular Expressions\n",
    "\n",
    "A regular expression is a sequence of characters that define a string.\n",
    "\n",
    "### 3.1 Simplest Form\n",
    "\n",
    "The simplest form of a regular expression is a sequence of characters contained within <b>two backslashes</b>. For example, <i>python</i> would be  \n",
    "\n",
    "``` \n",
    "\\python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Case Sensitivity\n",
    "\n",
    "Regular Expressions are <b>case sensitive</b>, which means \n",
    "\n",
    "``` \n",
    "\\p and \\P\n",
    "```\n",
    "are distinguishable from eachother. This means <i>python</i> and <i>Python</i> would have to be represented differently, as follows: \n",
    "\n",
    "``` \n",
    "\\python and \\Python\n",
    "```\n",
    "\n",
    "We can check these are different by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re1 = re.compile('python')\n",
    "print(bool(re1.match('Python')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Disjunctions\n",
    "\n",
    "If you want a regular expression to represent both <i>python</i> and <i>Python</i>, however, you can use <b>brackets</b> or the <b>pipe</b> symbol as the disjunction of the two forms. For example, \n",
    "\n",
    "``` \n",
    "[Pp]ython or \\Python|python\n",
    "```\n",
    "\n",
    "could represent either <i>python</i> or <i>Python</i>. Likewise, \n",
    "\n",
    "``` \n",
    "[0123456789]\n",
    "```\n",
    "\n",
    "would represent a single integer digit. The pipe symbols are typically used for interchangable strings, such as in the following example:\n",
    "\n",
    "```\n",
    "\\dog|cat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Ranges\n",
    "\n",
    "If we want a regular expression to express the disjunction of a range of characters, we can use a <b>dash</b>. For example, instead of the previous example, we can write \n",
    "\n",
    "``` \n",
    "[0-9]\n",
    "```\n",
    "Similarly, we can represent all characters of the alphabet with \n",
    "\n",
    "``` \n",
    "[a-z]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Exclusions\n",
    "\n",
    "Brackets can also be used to represent what an expression <b>cannot</b> be if you combine it with the <b>caret</b> sign. For example, the expression \n",
    "\n",
    "``` \n",
    "[^p]\n",
    "```\n",
    "represents any character, special characters included, but p.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
